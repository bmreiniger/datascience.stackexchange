{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyW85zVmcdeRJwxnyla5O7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bmreiniger/datascience.stackexchange/blob/master/SO79361226.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AEq-DRwGxUQ",
        "outputId": "ab90310d-85b3-4adb-94d0-29f73f8d7373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 140\n",
            "[LightGBM] [Info] Number of data points in the train set: 80, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 1.061487\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Difference between reg_y_hat and self_y_hat: 0.17329262573332693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate some random regression data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)\n",
        "y = 4 * X[:, 0] - 2 * X[:, 1] + np.random.rand(100) * 0.1\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the LGBMRegressor\n",
        "model = lgb.LGBMRegressor(objective='regression', n_estimators=2, learning_rate=0.1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Regular predict:\n",
        "reg_y_hat = model.predict(X_test)\n",
        "\n",
        "# Get the initial prediction (mean of y_train)\n",
        "init_pred = np.mean(y_train)\n",
        "\n",
        "# Get the train leaf values\n",
        "train_leaf_indices = model.predict(X_train, pred_leaf=True)\n",
        "leaf_samples = {(i, leaf_id): [] for i in range(model.n_estimators) for leaf_id in np.unique(train_leaf_indices[:, i])}\n",
        "\n",
        "# Store corresponding target values for each leaf\n",
        "for i, row in enumerate(train_leaf_indices):\n",
        "    for j, leaf_id in enumerate(row):\n",
        "        leaf_samples[(j, leaf_id)].append(y_train[i])\n",
        "\n",
        "# Compute avg for each leaf:\n",
        "leaf_agg = {}\n",
        "for key, values in leaf_samples.items():\n",
        "    leaf_agg[key] = np.mean(values)\n",
        "\n",
        "# Predict by aggregating the mean values and adding the initial prediction:\n",
        "preds = []\n",
        "test_leaf_indices = model.predict(X_test, pred_leaf=True)\n",
        "for row_indices in test_leaf_indices:\n",
        "    row_pred = init_pred\n",
        "    for i, leaf_index in enumerate(row_indices):\n",
        "        row_pred += model.learning_rate * (leaf_agg[(i, leaf_index)] - init_pred) # only the residual contribution of the leaf after initial prediction\n",
        "    preds.append(row_pred)\n",
        "self_y_hat = np.array(preds)\n",
        "\n",
        "# Verify the results\n",
        "print('Difference between reg_y_hat and self_y_hat:', np.abs(reg_y_hat - self_y_hat).sum())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate some random regression data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)\n",
        "y = 4 * X[:, 0] - 2 * X[:, 1] + np.random.rand(100) * 0.1\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the LGBMRegressor\n",
        "model = lgb.LGBMRegressor(objective='regression', n_estimators=10, learning_rate=0.1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Regular predict:\n",
        "reg_y_hat = model.predict(X_test)\n",
        "\n",
        "# Get the initial prediction (mean of y_train)\n",
        "init_pred = np.mean(y_train)\n",
        "\n",
        "# Predict by aggregating the mean values and adding the initial prediction:\n",
        "preds = []\n",
        "test_leaf_indices = model.predict(X_test, pred_leaf=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZNTqIcmGyZN",
        "outputId": "d98e6f81-0d6a-4aae-c335-45f64937d0c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 140\n",
            "[LightGBM] [Info] Number of data points in the train set: 80, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 1.061487\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_leaf_indices.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hHF_Hk0HfRI",
        "outputId": "0eca44a0-2a36-4137-855b-827645530ea4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_leaf_indices[0, :]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBAnuMmUIQmp",
        "outputId": "b7030190-12d7-43f6-dcc1-12e81b183845"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, 2, 1, 0, 1, 0, 1, 1], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = 0 # not init_pred, b/c init_pred is included in the first tree (!?)\n",
        "for tree_idx, leaf_idx in enumerate(test_leaf_indices[3, :]):\n",
        "    leaf_val = model.booster_.get_leaf_output(tree_idx, leaf_idx)\n",
        "    print(tree_idx, leaf_idx, leaf_val)  #trees_df.loc[trees_df['node_index'] == f'{tree_idx}-L{leaf_idx}', 'value'].item())\n",
        "    val += leaf_val\n",
        "val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9_cSpciH52T",
        "outputId": "68e2a0f9-bb55-4fbe-f926-9ea7cd2fe9b0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 0.9294197992534574\n",
            "1 0 -0.1465100972602765\n",
            "2 0 -0.1318590881302953\n",
            "3 0 -0.11867317873984576\n",
            "4 0 -0.08845862099957286\n",
            "5 2 -0.09883328131758251\n",
            "6 0 -0.07400195739711776\n",
            "7 2 -0.08406647059779901\n",
            "8 0 -0.08110933775897139\n",
            "9 0 -0.061811353465600405\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.044096413586395905"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G7akGLrJByq",
        "outputId": "d42eceaa-60bf-473c-aefc-3fd17a9464b8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.52392002, 1.56749652, 0.84895338, 0.04409641, 1.56749652,\n",
              "       0.4562928 , 0.22088663, 0.04409641, 1.56749652, 0.4562928 ,\n",
              "       0.04409641, 2.06567294, 0.51469892, 2.06567294, 1.56749652,\n",
              "       1.66122078, 0.04409641, 2.06567294, 0.85290376, 0.04409641])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row_indices in test_leaf_indices:\n",
        "    row_pred = 0\n",
        "    for tree_index, leaf_index in enumerate(row_indices):\n",
        "        row_pred += model.booster_.get_leaf_output(tree_index, leaf_index)\n",
        "    preds.append(row_pred)\n",
        "self_y_hat = np.array(preds)\n",
        "\n",
        "# Verify the results\n",
        "print('Difference between reg_y_hat and self_y_hat:', np.abs(reg_y_hat - self_y_hat).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kuTHlYeHdgZ",
        "outputId": "116d62ef-a9d0-4bed-a38a-fec3b2862e89"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between reg_y_hat and self_y_hat: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yuCxPCIVLLX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}